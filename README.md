# AgentEval: Corporate Governance Scorecard with Agentic AI

A sophisticated multi-agent AI system for automated corporate governance evaluation using Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG).

## ğŸ¯ Overview

AgentEval transforms corporate governance assessment through a specialized five-agent architecture that processes unstructured corporate documents to generate standardized governance scores with detailed justifications and source citations.

## âœ¨ Key Features

- **Multi-Agent Architecture**: Specialized agents for input validation, question generation, research, output validation, and scoring
- **Hybrid Retrieval System**: Combines BM25 keyword search with semantic embeddings for comprehensive document analysis
- **Pre-Computed Optimization**: 20-100x speed improvement through cached chunks and embeddings
- **PDF Slice Reconstruction**: Preserves document structure including tables and charts
- **Intelligent Fallback**: Progressive escalation when retrieval methods find insufficient information
- **Quality Guardrails**: Deterministic validation and hallucination detection
- **Explainable Scoring**: Detailed justifications with preserved source citations

## ğŸ—ï¸ Architecture

```
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚ Input Guardrail â”‚
                          â”‚ Agent           â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚ validates topic
                                    â–¼
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚ Question Agent  â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚                 â”‚          â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
                                    â”‚ generates questionâ”‚ generates follow-up
                                    â–¼                  â”‚ if gaps found
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
                          â”‚ Research Agent  â”‚          â”‚
                          â”‚                 â”‚          â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
                                    â”‚ produces answer   â”‚
                                    â–¼                  â”‚
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
                          â”‚ Output Guardrailâ”‚          â”‚
                          â”‚ Agent           â”‚          â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
                                    â”‚ validates answer  â”‚
                                    â–¼                  â”‚
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
                          â”‚ Evidence Pool   â”‚          â”‚
                          â”‚                 â”‚          â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
                                    â”‚ check completenessâ”‚
                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚ if sufficient evidence
                                    â–¼
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚ Scoring Agent   â”‚
                          â”‚                 â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8+
- Google API Key (for Gemini models) or Ollama (for local models)
- Required packages: `langchain`, `chromadb`, `sentence-transformers`, `streamlit`

### Installation

```bash
git clone https://github.com/your-username/agentevaluation
cd agentevaluation
pip install -r requirements.txt
```

### Environment Setup

```bash
# Create .env file
echo "GOOGLE_API_KEY=your_api_key_here" > .env
```

### Directory Structure

```
./data/COMPANY_NAME/
â”œâ”€â”€ 98_data/           # Place PDF documents here
â”œâ”€â”€ 97_cache/          # Auto-generated cache files
â””â”€â”€ 96_results/        # Evaluation results
```

### Basic Usage

```python
from main import OptimizedConfig, OptimizedAgenticOrchestrator, TopicDefinition

# Configure system
config = OptimizedConfig("YOUR_COMPANY")
orchestrator = OptimizedAgenticOrchestrator(config)

# Define evaluation topic
topic = TopicDefinition(
    topic_name="Board Independence",
    goal="Assess board director permanence",
    guidance="Analyze appointment dates...",
    scoring_rubric={
        "0": "Permanent directors present",
        "1": "Permanent directors are lender reps",
        "2": "All directors non-permanent"
    }
)

# Run evaluation
result = orchestrator.evaluate_topic(topic)
print(f"Score: {result['scoring']['score']}/2")
```

### Web Interface

```bash
streamlit run app.py
```

## ğŸ“Š Usage Examples

### Command Line Interface

```bash
# Test all retrieval methods
python main.py --test-all

# Test specific method
python main.py --method hybrid

# Performance comparison
python main.py --test-performance
```

### Programmatic Configuration

```python
# Customize retrieval method
config.retrieval_method = "hybrid"  # "bm25", "vector", "direct"

# Configure agent-specific LLMs
config.agent_llms = {
    "research_agent": "gemini-1.5-pro",
    "scoring_agent": "gemini-1.5-flash"
}

# Set agent temperatures
config.agent_temperatures = {
    "input_agent": 0.1,     # Consistent validation
    "research_agent": 0.2,  # Factual analysis
    "scoring_agent": 0.1    # Deterministic scoring
}
```

## ğŸ”§ Configuration Options

| Parameter | Description | Default |
|-----------|-------------|---------|
| `retrieval_method` | Search strategy | `"hybrid"` |
| `max_iterations` | Research cycles | `3` |
| `use_pdf_slices` | PDF reconstruction | `True` |
| `chunk_size` | Text chunk size | `1000` |
| `temperature` | LLM creativity | `0.2` |

## ğŸ“ˆ Performance Metrics

- **Speed**: 20-100x faster than traditional approaches
- **Accuracy**: High fidelity against expert assessments
- **Consistency**: Deterministic scoring with low variance
- **Coverage**: Comprehensive source identification and citation

## ğŸ›¡ï¸ Quality Assurance

- **Input Validation**: Topic definition completeness and coherence
- **Output Validation**: Answer quality, citations, and confidence assessment
- **Fallback Mechanisms**: Progressive document escalation when needed
- **Error Handling**: Graceful degradation and comprehensive logging

## ğŸ“ Project Structure

```
â”œâ”€â”€ main.py              # Core agentic system
â”œâ”€â”€ app.py               # Streamlit web interface
â”œâ”€â”€ requirements.txt     # Python dependencies
â”œâ”€â”€ .env.example         # Environment template
â””â”€â”€ data/               # Document storage
    â””â”€â”€ COMPANY/
        â”œâ”€â”€ 98_data/    # Input PDFs
        â”œâ”€â”€ 97_cache/   # Cached processing
        â””â”€â”€ 96_results/ # Evaluation outputs
```

## ğŸ”„ Agent Workflow

### Iterative Research Process

1. **Input Validation**: Validates topic definition structure and semantics
2. **Initial Question**: Generates strategic question based on rubric analysis
3. **Research Loop** (up to 3 iterations):
   - Research Agent analyzes documents using hybrid retrieval
   - Output Guardrail validates answer quality
   - Question Agent checks if more information needed
   - If gaps found, generates follow-up question
   - Loop continues until sufficient evidence or max iterations
4. **Final Scoring**: Synthesizes all evidence for final score and justification

### Agent Specializations

- **Input Guardrail**: Rule-based + LLM validation (Temperature: 0.1)
- **Question Agent**: Strategic question generation (Temperature: 0.3)
- **Research Agent**: Document analysis with fallback mechanisms (Temperature: 0.2)
- **Output Guardrail**: Deterministic quality validation (No LLM)
- **Scoring Agent**: Evidence synthesis and scoring (Temperature: 0.1)

## ğŸ§ª Testing

```bash
# Quick test with default settings
python main.py

# Test specific company data
python main.py --company COMPANY_NAME

# Benchmark different methods
python main.py --test-all

# Performance analysis
python main.py --test-performance
```

## ğŸ”§ Advanced Configuration

### Custom LLM Models

```python
config.agent_llms = {
    "input_agent": "gemini-1.5-flash",
    "question_agent": "gemini-1.5-flash", 
    "research_agent": "gemini-1.5-pro",
    "scoring_agent": "gemini-1.5-flash"
}
```

### Retrieval Method Selection

- **hybrid**: Best balance of accuracy and coverage (default)
- **bm25**: Fast keyword-based search
- **vector**: Semantic similarity search
- **direct**: Full document analysis (slower but comprehensive)

### Performance Tuning

```python
config.max_iterations = 3           # Research cycles
config.chunk_size = 1000           # Text chunk size
config.chunk_overlap = 200         # Overlap between chunks
config.max_pdf_size_mb = 20        # PDF size limit for direct processing
config.force_recompute = False     # Clear cache and recompute
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/enhancement`)
3. Commit changes (`git commit -m 'Add enhancement'`)
4. Push to branch (`git push origin feature/enhancement`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ“š Citation

```bibtex
@article{shah2024agentevaluation,
  title={AgentEval: Corporate Governance Scorecard with Agentic AI},
  author={Shah, Monil and Jadhav, Supriya},
  journal={Northwestern University},
  year={2024}
}
```

## ğŸ“ Support

- **Issues**: [GitHub Issues](https://github.com/your-username/agentevaluation/issues)
- **Documentation**: See inline code documentation
- **Contact**: {Monilshah2025, SupriyaJadhav2025}@u.northwestern.edu

## ğŸ† Research Highlights

- **First multi-agent system** specifically designed for corporate governance evaluation
- **20-100x performance improvement** through pre-computation optimization
- **Hybrid retrieval innovation** combining keyword and semantic search
- **Quality assurance framework** with dedicated guardrail agents
- **Explainable AI** with detailed justifications and source citations

---

**Built with â¤ï¸ by Northwestern University Research Team**
